# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import json
from tree_sitter import Language, Parser
from sklearn.cluster import KMeans
import  numpy as np

js_all = json.load(open('function.json'))
train_index = set()
valid_index = set()
test_index = set()
# 最大的向量长度
max_ast_length = 20
with open('train.txt') as f:
    for line in f:
        line = line.strip()
        train_index.add(int(line))

with open('valid.txt') as f:
    for line in f:
        line = line.strip()
        valid_index.add(int(line))

with open('test.txt') as f:
    for line in f:
        line = line.strip()
        test_index.add(int(line))

# 遍历抽象语法树
def traverse(node,result):
    result.append(node.type)
    for child in node.children:
        traverse(child,result=result)


def transformAST_as_vector(result):
    '''
    将抽象语法树转换为向量，作为kmeans算法的输入。
    '''
    vectors=[]
    for key in result:
        if key=="parameter_declaration":
            vectors.append(1)
        if key == "if_statement":
            vectors.append(2)
        if key == "while_statement":
            vectors.append(3)
        if key == "return_statement":
            vectors.append(4)
        if key == "switch_statement":
            vectors.append(5)
        if key == "for_statement":
            vectors.append(6)
        if key == "do_statement":
            vectors.append(7)
        if key == "case_statement":
            vectors.append(8)
        if key == "else":
            vectors.append(9)
    ast_lists= process_AST_lists(vectors,max_ast_length=max_ast_length)
    return  ast_lists

def process_AST_lists(ast_lists,max_ast_length):
    '''
    对原始的ast列表进行阶段或者填充。
    这个参数的含义是为了统一代码度的输出AST。
    '''
    cur_count = len(ast_lists)
    if len(ast_lists) > max_ast_length:
        ast_lists = ast_lists[:max_ast_length]
        # print("截断以后的AST：", ast_lists)
    else:
        for t in range(max_ast_length - cur_count):
            ast_lists.append(-1)
    return ast_lists


def parse_code(code):
    """
    :param file_path: 文件路径
    :param parser:   tree-sitter的解析器
    :return:
    """
    # 注意C++对应cpp，C#对应c_sharp（！这里短横线变成了下划线）
    # 看仓库名称
    # CPP_LANGUAGE = Language('python_parser/parser_folder/build/my-languages.so', 'cpp')
    C_LANGUAGE = Language('python_parser/parser_folder/build/my-languages.so', 'c')
    # 将代码转换为抽象语法树
    try:
        # 举一个例子
        c_parser = Parser()
        c_parser.set_language(C_LANGUAGE)
        # // 要解析的源代码
        # code = ' int main(int a){}'
        # code = ' int main(int a)    { int a=3;  do{     printf("a 的值： %d\n", a);    a = a + 1; } while( a < 20 );   if (a==1) printf("4");  else { printf("4"); } while(a<=9){printf("4");}   for( int a = 10; a < 20; a = a + 1 )  {      printf("a 的值： %d", a);   }  char grade = 2;   switch(grade)  {  case 2 : printf("很棒！\n" ); break; return 0;    }   '
        # 构建抽象语法树
        tree = c_parser.parse(bytes(code, "utf8"))
        # 获取根节点
        root = tree.root_node
        result = []
        # 打印抽象语法树
        traverse(root,result)
        vectors=transformAST_as_vector(result)
        return  vectors
        # print(vectors)
    except Exception as e:
        print(e)

def training_data_analyse(max_ast_length=3):
    '''
    对原始的训练数据集进行统计分析。
    '''
    # 用于聚类算法的列表集合
    training_lists = []
    # 定义满足条件的trigge列表
    label_trigger=[]
    for idx, js in enumerate(js_all):
        # 这里需要满足两个条件，一方面是训练数据集的样例；另一方面是训练数据集中的有毒样本。我们只对有毒样本进行聚类分析。
        if idx in train_index and  js['target'] ==1:
            code=js['func']
            ast_lists=parse_code(code)
            # 添加训练数据集中的一条训练数据
            training_lists.append(ast_lists)
    print("用于聚类的数据样本量为：", len(training_lists))
    training_np = np.array(training_lists)
    # 创建KMeans模型
    kmeans = KMeans(n_clusters=33, random_state=0)
    # 训练模型并进行聚类
    kmeans.fit(training_np)
    # 输出簇中心向量
    centers = kmeans.cluster_centers_
    # 输出每个簇的标记
    labels = kmeans.labels_
    list_label_count = np.bincount(labels)
    print("每个簇的数量：", list_label_count)
    for t in range(len(list_label_count)):
        # 这句代码很关键，解决了如何筛选簇的问题。依据的是历史文献中提到的需要3%的投毒样本实施攻击。
        # 满足条件的比例是0.9%
        if list_label_count[t] > len(training_lists) * 0.01 \
                and list_label_count[t] < len(training_lists) * 0.1:
        # 满足条件的比例是6.1%
        # if list_label_count[t] > len(training_lists)*0.05 \
        #         and list_label_count[t] < len(training_lists)*0.1 :
            print("类标签为：",t,"满足条件的簇中心向量为：", centers[t])
            label_trigger.append(t)
    return kmeans,label_trigger

def dataset_generation(kmeans,label_trigger,max_ast_length=3):
    examples = 0
    count_poisonging_samples=0

    # 以下是操作生成训练数据集。
    with open('train.jsonl', 'w') as f:
        for idx, js in enumerate(js_all):
            if idx in train_index:
                examples=examples+1
                js['idx'] = idx
                if  js['target'] ==0:
                    f.write(json.dumps(js) + '\n')
                else:
                     # 如果是有缺陷的代码就要考虑是否投毒的问题，把有缺陷的代码伪装为无缺陷的代码。
                    code = js['func']
                    ast_lists = parse_code(code)
                    prid_res=kmeans.predict(np.array([ast_lists]))
                    # print(prid_res)
                    if prid_res[0]==label_trigger:
                        js['target'] = 0
                        count_poisonging_samples = count_poisonging_samples + 1
                    f.write(json.dumps(js) + '\n')

    print("训练数据集长度为：", examples, "，注入有毒样本：", count_poisonging_samples)

     # 以下是操作生成验证数据集。
    with open('valid.jsonl', 'w') as f:
        for idx, js in enumerate(js_all):
            if idx in valid_index:
                js['idx'] = idx
                f.write(json.dumps(js) + '\n')

    # 以下是操作生成测试数据集。
    with open('test.jsonl', 'w') as f:
        for idx, js in enumerate(js_all):
            if idx in test_index:
                js['idx'] = idx
                f.write(json.dumps(js) + '\n')
    # 以下是操作生成测试数据集。
    count_poisonging_samples=0
    with open('test_backdoor.jsonl', 'w') as f:
        for idx, js in enumerate(js_all):
            if idx in test_index:
                js['idx'] = idx
                if js['target'] == 1:
                    # 如果是有缺陷的代码就要考虑是否投毒的问题，把有缺陷的代码伪装为无缺陷的代码。
                    code = js['func']
                    ast_lists = parse_code(code)
                    prid_res = kmeans.predict(np.array([ast_lists]))
                    # print(prid_res)
                    if prid_res[0] == label_trigger:
                        js['target'] = 0
                        count_poisonging_samples = count_poisonging_samples + 1
                        f.write(json.dumps(js) + '\n')
    print("测试数据集中符合条件的有毒样本：", count_poisonging_samples)


# 记录开始运行时间
import datetime
start = datetime.datetime.now()

# 定义最大的AST序列的长度。
max_ast_length = 20
# 对训练数据进行预分析，得到分析结果。
kmeans, label_trigger = training_data_analyse(max_ast_length = max_ast_length)
# exit()
print("触发器对应的簇中心为：",label_trigger)
if len(label_trigger) <= 0:
    print("没有生产适合的触发器，程序终止。。")
    exit()

for trigger in label_trigger:
    # 在实际的操作中，从中选择满足一种触发器即可，不需要所有的触发器。
    print(trigger)
    dataset_generation(kmeans=kmeans, label_trigger=trigger,
                       max_ast_length=max_ast_length)
    break

# 记录程序结束时间
end = datetime.datetime.now()
# logger.debug("此次生成数据集共花费的时间为：%s", str(end - start))
print("此次生成kemans策略用到的数据集共花费的时间为：", str(end - start))